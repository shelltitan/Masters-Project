{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Code Outline -------- #\n",
    "# Create a model that implements an RNN using output predictions from a CNN in order to identify rhythm changes\n",
    "# We implement a stratified 10 k fold validation scheme to analyse how well the model will perform on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adb de-noised/100_de-noised.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "end = len(y)\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have time series data of labels for each patient. These series will be different lengths so we need to\n",
    "# Pad them all to add 0's to the end of each sequence to get longer length inputs. So first we save how many labels are\n",
    "# In each patient as well as how many beats are in the signal.\n",
    "\n",
    "# Load in the beat labels for each patient as well as the rhythm labels for each\n",
    "\n",
    "patient_id = range(0,48)\n",
    "\n",
    "beat_labels = []\n",
    "\n",
    "beat_locations = []\n",
    "\n",
    "rhythm_labels = []\n",
    "\n",
    "rhythm_locations = []\n",
    "\n",
    "full_test = [14,24]\n",
    "half_test = [4,28,40,41,44,45]\n",
    "\n",
    "test_beat_labels = []\n",
    "test_beat_locations = []\n",
    "test_rhythm_labels = []\n",
    "test_rhythm_locations = []\n",
    "\n",
    "half_beat_labels = []\n",
    "half_beat_locations = []\n",
    "half_rhythm_labels = []\n",
    "half_rhythm_locations = []\n",
    "\n",
    "for i in patient_id:\n",
    "    # Beat labels for patient i\n",
    "    with open('adb final labels/adb beat labels/{}_beat_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat = pickle.load(f)\n",
    "    # beat label locations for patient i\n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat_locations = pickle.load(f)\n",
    "    # rhythm labels for patient i\n",
    "    with open('adb final labels/adb rhythm labels/{}_rhythm_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm = pickle.load(f)\n",
    "    # Rhythm label locations for patient i\n",
    "    with open('adb final labels/adb rhythm locations/{}_rhythm_locations.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm_location = pickle.load(f)\n",
    "        \n",
    "    # Split up into test and training\n",
    "    if(i in full_test):\n",
    "        test_rhythm_locations.append(temp_rhythm_location)\n",
    "        test_beat_labels.append(temp_beat)\n",
    "        test_rhythm_labels.append(temp_rhythm)\n",
    "        test_beat_locations.append(temp_beat_locations)\n",
    "    elif(i in half_test):\n",
    "        half_rhythm_locations.append(temp_rhythm_location)\n",
    "        half_beat_labels.append(temp_beat)\n",
    "        half_rhythm_labels.append(temp_rhythm)\n",
    "        half_beat_locations.append(temp_beat_locations)\n",
    "    else:\n",
    "        rhythm_locations.append(temp_rhythm_location)\n",
    "        beat_labels.append(temp_beat)\n",
    "        rhythm_labels.append(temp_rhythm)\n",
    "        beat_locations.append(temp_beat_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(rhythm_labels))\n",
    "#print(half_rhythm_labels[-1])\n",
    "#print(len(test_rhythm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(beat_labels[32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the half patient test data into half in order to get representative test set\n",
    "index = []\n",
    "for j in half_beat_locations:\n",
    "    # Find beat location nearest to the halfway point\n",
    "    index.append(min(enumerate(j), key=lambda x: abs(x[1]-(end/2)))[0])\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest rhythm location to the beat\n",
    "def find_beat_to_rhythm(beat_location, rhythm_location):\n",
    "    index, value = min(enumerate(rhythm_location), key=lambda x: abs(x[1]-(beat_location)))\n",
    "    return index, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have found the beat corresponding to the halfway point of the ecg, and then the function above finds the last rhythm\n",
    "# Label before this point\n",
    "\n",
    "# For every halfway beat in the list above we now need to find the nearest rhythm to it and use that as our starting rhythm\n",
    "\n",
    "for c,k in enumerate(index):\n",
    "    starting_rhythm_index, starting_rhythm_value = find_beat_to_rhythm(half_beat_locations[c][k], half_rhythm_locations[c])\n",
    "    halfway_list_locations = [end / 2]\n",
    "    halfway_list_labels = [half_rhythm_labels[c][starting_rhythm_index]]\n",
    "    \n",
    "    if(c == 0):\n",
    "        # We want left half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][:starting_rhythm_index])\n",
    "        temp_list = half_rhythm_locations[c][:starting_rhythm_index] + [end / 2]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][:k])\n",
    "        test_beat_locations.append(half_beat_locations[c][:k])\n",
    "        # Save the right half into the training set\n",
    "        temp_labels = halfway_list_labels + half_rhythm_labels[c][(starting_rhythm_index):]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        temp_locations = halfway_list_locations + half_rhythm_locations[c][(starting_rhythm_index):]\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][k:])\n",
    "        beat_locations.append(half_beat_locations[c][k:])\n",
    "    else:\n",
    "        # We want right half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][(starting_rhythm_index - 1):])\n",
    "        temp_list =  [end / 2] + half_rhythm_locations[c][starting_rhythm_index:]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][k:])\n",
    "        test_beat_locations.append(half_beat_locations[c][k:])\n",
    "        # Save the left half into the training set\n",
    "        if(len(half_rhythm_labels[c]) == 1): # If we only have one rhythm present in the sample\n",
    "            temp_labels = [half_rhythm_labels[c][0]]\n",
    "            temp_locations = [half_rhythm_locations[c][0]]\n",
    "        else:\n",
    "            temp_labels = half_rhythm_labels[c][:starting_rhythm_index]\n",
    "            temp_locations = half_rhythm_locations[c][:starting_rhythm_index]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][:k])\n",
    "        beat_locations.append(half_beat_locations[c][:k])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(half_beat_locations[0][index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(half_beat_locations[0])\n",
    "#print(half_beat_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_beat_locations[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test beat locations, rhythm labels and locations for later use\n",
    "with open('Test Data/Beat Locations.pkl', 'wb') as f:\n",
    "    pickle.dump(test_beat_locations,f)\n",
    "with open('Test Data/Rhythm Locations.pkl', 'wb') as f:\n",
    "    pickle.dump(test_rhythm_locations,f)\n",
    "with open('Test Data/Rhythm Labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_rhythm_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_beat_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to remove the last beat from the arrays as this is usually erroneous due to disconnection of ECG\n",
    "# Can only do this for the latter half of each training set but for the first half of patients [28,40,41,44,45] we have only\n",
    "# The left hand side of the ECG so the last labels etc are fine. We appended these on last so the last 5 patients do not need\n",
    "# Changing\n",
    "\n",
    "for i in beat_locations:\n",
    "    del i[-1]\n",
    "for j in beat_labels:   \n",
    "    del j[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#beat_locations[6][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the starting and ending points of all the rhythms, we need to create windows of pre-determined\n",
    "# Sample size and then these will be given a target label in the form of a vector. This vector will tell us which \n",
    "# Rhythms are present within the sample.\n",
    "\n",
    "# Choose a sample size - The number of beat labels you are going to send in\n",
    "sample_size = 10\n",
    "\n",
    "# Now we need to create sublists for each patient with 10 beat labels in\n",
    "full_list = []\n",
    "for j in beat_labels:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(len(full_list[0]))\n",
    "#print(full_list[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to replace each type with their corresponding integer number\n",
    "integer_samples = [[[0 if b == 'N'\n",
    "          else 1 if b == 'L'\n",
    "          else 2 if b == 'R'\n",
    "          else 3 if b == 'A'\n",
    "          else 4 if b == 'a'\n",
    "          else 5 if b == 'J'\n",
    "          else 6 if b == 'S'\n",
    "          else 7 if b == 'V'\n",
    "          else 8 if b == 'F'\n",
    "          else 9 if b == '!'\n",
    "          else 10 if b == 'e'\n",
    "          else 11 if b == 'j'\n",
    "          else 12 if b == 'E'\n",
    "          else 13 if b == '/'\n",
    "          else 14 if b == 'f'\n",
    "          else 15 if b == 'x'\n",
    "          else 16 if b == 'Q'\n",
    "          else 17 for b in j] for j in k] for k in full_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[0][90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(full_list[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some of the samples will be missing beats so for these we need to pad them with arbitrary values \n",
    "# To make them 10 in length\n",
    "\n",
    "# Loop over the samples and pad the last elements\n",
    "for i in integer_samples:\n",
    "    for c,j in enumerate(i):\n",
    "        if (c == (len(i) - 1)):\n",
    "            # Pad last sample\n",
    "            element_to_add = sample_size - len(j)\n",
    "            for k in range(element_to_add):\n",
    "                new = [99]\n",
    "                j = j + new\n",
    "            i[c] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have loads of samples in integer format with the encoding from above. We now need to go through and\n",
    "# Create new vector label arrays of dimension (15,) in form [1,0,0,0.....] if it is a Atrial bigeminy\n",
    "# [0,1,0,0,0,0...] if it is a Atrial fibrillation etc. If the sample contains a mixture of rhythms then we opt for\n",
    "# A label such as [1,1,0,0,0....] for AF and atrial bigeminy\n",
    "\n",
    "# Now we need to create sublists for each patient with 25 beat label locations in\n",
    "location_list = []\n",
    "for j in beat_locations:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    location_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rhythm_locations[-2])\n",
    "#print(location_list[-1][-1])\n",
    "#print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to loop over every sample and check where the beat labels fit into with regards to the rhythm ranges\n",
    "# For ease we will assume the first few samples up the rhythm label are also the same rhythm.\n",
    "# Rhythm labels:\n",
    "# AB - atrial bigeminy (0), AFIB - atrial fibrillation(1), AFL - atrial flutter(2), B - ventricular bigeminy(3)\n",
    "# BII - 2 heart block(4), IVR - idioventricular rhythm(5), N - normal sinus rhythm(6), NOD - nodal rhythm(7)\n",
    "# P - paced rhythm(8), PREX - pre-excitation(9), SBR - sinus brachycardia(10), SVTA - supraventricular tachyarrhymia(11)\n",
    "# T - ventricular trigeminy(12), VFL - ventricular flutter(13), VT - ventricular tachycardia(14)\n",
    "\n",
    "sample_labels = []\n",
    "\n",
    "# First pick a patient\n",
    "for i,patient in enumerate(rhythm_locations):\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    patient_beat_labels = []\n",
    "    \n",
    "    # Now loop over all the beat labels in that patients data\n",
    "    for beat_location in beat_locations[i]:\n",
    "        \n",
    "        # Loop over all the rhythms and find which one it is after and use that rhythm label\n",
    "        \n",
    "        rhythm_after = []\n",
    "        \n",
    "        for c,rhythm_location in reversed(list(enumerate(patient))):\n",
    "            if (len(rhythm_after) > 0):\n",
    "                break\n",
    "            else:\n",
    "                if(beat_location > rhythm_location):\n",
    "                    rhythm_after.append(rhythm_labels[i][c])\n",
    "                    #print(rhythm_labels[i][c])\n",
    "                \n",
    "        patient_beat_labels.append(rhythm_after)\n",
    "        #print(patient_beat_labels[-1])\n",
    "       # print(beat_location)\n",
    "        #print(i)\n",
    "        \n",
    "    sample_labels.append(patient_beat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sample_labels[-1])\n",
    "#print(len(beat_locations[8]))\n",
    "#print(beat_locations[8][90:110])\n",
    "#print(rhythm_locations[8])\n",
    "#print(rhythm_labels[8][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the rhythms set up we need to segment into blocks of sample_size again\n",
    "\n",
    "full_list_labels = []\n",
    "for j in sample_labels:\n",
    "    samples_labels = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list_labels.append(samples_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "# Per patient\n",
    "for c,i in enumerate(full_list_labels):\n",
    "    #print(c)\n",
    "    patient = []\n",
    "    # Each sample\n",
    "    for j in i:\n",
    "        new = []\n",
    "        # Get rid of annoying list notation\n",
    "        for k in j:\n",
    "            try:\n",
    "                new.append(k[0])\n",
    "            except IndexError:\n",
    "                print(k)\n",
    "            #new.append(k[0])\n",
    "        patient.append(new)\n",
    "    x.append(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x[-2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(integer_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an array of sample_labels as well as a boolean ~(tells us if there is a mixture or not)~\n",
    "# We then create a corresponding vector for that sample\n",
    "def create_complete_label(sample_labels):\n",
    "\n",
    "    # Doing this finds the UNIQUE elements of the sample_labels\n",
    "    unique_elements = list(set(sample_labels))\n",
    "    \n",
    "    #print(unique_elements)\n",
    "    #for i in unique_elements:\n",
    "        #print(i)\n",
    "        \n",
    "    # Create vector - some reason they have a bracket in front of all of them when you extract them\n",
    "    label = [0 if b == 'AB'\n",
    "        else 1 if b == 'AFIB'\n",
    "        else 2 if b == 'AFL'\n",
    "        else 3 if b == 'B'\n",
    "        else 4 if b == 'BII'\n",
    "        else 5 if b == 'IVR'\n",
    "        else 6 if b == 'N'\n",
    "        else 7 if b == 'NOD'\n",
    "        else 8 if b == 'P'\n",
    "        else 9 if b == 'PREX'\n",
    "        else 10 if b == 'SBR'\n",
    "        else 11 if b == 'SVTA'\n",
    "        else 12 if b == 'T'\n",
    "        else 13 if b == 'VFL'\n",
    "        # This one is VT label\n",
    "        else 14 for b in unique_elements]\n",
    "        \n",
    "    # Return the vector\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multilabel vector for each window\n",
    "\n",
    "complete_labels = []\n",
    "for i in x:\n",
    "    patient_l = []\n",
    "    for j in i:\n",
    "        patient_l.append(create_complete_label(j))\n",
    "        \n",
    "    complete_labels.append(patient_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(complete_labels[-1])\n",
    "#print(rhythm_labels[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transfer these into vectors like [1,0,0,0,0,0,0,0,0,0...] if label 1 is present or\n",
    "# [1,1,0,0,0,0,0,0,0....] if a mixture of label 1 or 2 are present etc.\n",
    "\n",
    "y = []\n",
    "\n",
    "# Loop over every sample and create a vector for it and append it to a list\n",
    "for c,patient in enumerate(complete_labels):\n",
    "    \n",
    "    for sample in patient:\n",
    "        \n",
    "        # Create a label vector\n",
    "        temp = np.zeros((15,))\n",
    "        \n",
    "        # Find which numbers are present, then these indices need to be set to 1\n",
    "        for item in sample:\n",
    "            temp[item] = 1\n",
    "            \n",
    "        y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have complete set of target labels and input arrays\n",
    "# Check\n",
    "#print(integer_samples[0])\n",
    "# Need to convert integer_samples to a 1 dimensional array of 10 sample windows\n",
    "#print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten input array into just 10 window rhythms\n",
    "x = []\n",
    "for i in integer_samples:\n",
    "    for j in i:\n",
    "        x.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(len(y), sample_size,1)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y[170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function that finds proportions of the different labels\n",
    "\n",
    "def proportions(labels):\n",
    "    \n",
    "    # Data is in vector form so [1,0,0,0,0....] etc with this indicating it is the first rhythm\n",
    "    # We need to loop over the vector to find where the 1's are then we can get the proportion that is a certain type\n",
    "    \n",
    "    n_mixed, n_pure, n_AB, n_AFIB, n_AFL, n_B, n_BII, n_IVR, n_N, n_NOD, n_P, n_PREX, n_SBR, n_SVTA, n_T, n_VFL, n_VT = 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    options = {0 : n_AB,\n",
    "           1 : n_AFIB,\n",
    "           2 : n_AFL,\n",
    "           3 : n_B,\n",
    "           4 : n_BII,\n",
    "           5 : n_IVR,\n",
    "           6 : n_N,\n",
    "           7 : n_NOD,\n",
    "           8 : n_P,\n",
    "           9 : n_PREX,\n",
    "          10 : n_SBR,\n",
    "          11 : n_SVTA,\n",
    "          12 : n_T,\n",
    "          13 : n_VFL,\n",
    "          14 : n_VT,\n",
    "          15 : n_mixed,\n",
    "          16 : n_pure\n",
    "}\n",
    "\n",
    "    for i in labels:\n",
    "        #print(i)\n",
    "        mix = False\n",
    "        count = 0\n",
    "        for index,number in enumerate(i):\n",
    "            #print(count)\n",
    "            #print(number)\n",
    "            \n",
    "            #if ((number == 1) and (index == 0)):\n",
    "                #print(i)\n",
    "            \n",
    "            if(number == 1):\n",
    "                count += 1\n",
    "            \n",
    "                # Change variable in dictionary to one up\n",
    "                options[index] += 1\n",
    "                \n",
    "            elif(number == 0):\n",
    "                count = count\n",
    "        \n",
    "        if(count >= 2):\n",
    "                mix = True\n",
    "                options[15] += 1\n",
    "        elif(count == 1):\n",
    "                mix = False\n",
    "                options[16] += 1\n",
    "                \n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = proportions(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_split)\n",
    "\n",
    "# This checks we have correct number of samples\n",
    "#print(data_split[15] + data_split[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data and training labels\n",
    "with open('RNN Training Data.pkl', 'wb') as f:\n",
    "    pickle.dump(x, f)\n",
    "with open('RNN Training Targets.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some labels are not present frequently in the sample, we need to implement a k fold validation scheme\n",
    "# In the hope that this will produce a more representative set.\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 28\n",
    "\n",
    "# Many thanks to creators of iterative stratification and scikit-multilearn\n",
    "# Reference :\n",
    "# If you use this method to stratify data please cite both:\n",
    "# 1 -> Sechidis, K., Tsoumakas, G., & Vlahavas, I. (2011). On the stratification of multi-label data. Machine Learning and Knowledge Discovery in Databases, 145-158. http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf\n",
    "# 2 -> Piotr SzymaÅ„ski, Tomasz Kajdanowicz ; Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications, PMLR 74:22-35, 2017. http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html\n",
    "# Found on http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html\n",
    "\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    " \n",
    "# The more splits you do the more likely the data is representative however this comes with trade off that\n",
    "# You will have a lower percentage of data in your test set as well as the fact that it will take significantly\n",
    "# Longer computationally. Thankfully the RNN is very fast as we are just using sequences so a 10-fold validation\n",
    "# Will be sufficient and leaves 90% of the data for training purposes\n",
    "\n",
    "n_split = 10\n",
    " \n",
    "k_fold = IterativeStratification(n_splits = n_split, order=1)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# This is just to check that splits are somewhat correct and then crucially saves the data set splits for each fold\n",
    "# Into a file for analysis later\n",
    "\n",
    "# Save the training and validation indices for each fold so that we can use them on the server\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    print(\"Fold\" + str(i))\n",
    "    temp_1 = proportions(y[train])\n",
    "    temp_2 = proportions(y[val])\n",
    "    print(\"Train set\")\n",
    "    print(temp_1)\n",
    "    print(\"Validation set\")\n",
    "    print(temp_2)\n",
    "    i += 1\n",
    "    train_indices.append(train)\n",
    "    val_indices.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the indices for GPU use on server as we cannot download the multilearn on there\n",
    "with open('RNN Training Indices.pkl', 'wb') as f:\n",
    "    pickle.dump(train_indices, f)\n",
    "with open('RNN Validation Indices.pkl', 'wb') as f:\n",
    "    pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folds are ready to go so now we need to train on every fold and save a couple of things:\n",
    "# 1. Data set for each fold\n",
    "# 2. Actual data for each fold\n",
    "# 3. Accuracies on validation and training set for each fold\n",
    "# 4. Average accuracy and loss over 10 folds\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix, hamming_loss, jaccard_score\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Need to save the scores for each fold evaluation\n",
    "scores = np.zeros((n_split, 4))\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    \n",
    "    print(\"Fold \" + str(i))\n",
    "    \n",
    "    train_x = x[train]\n",
    "    train_y = y[train]\n",
    "    val_x = x[val]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    # Find new proportions and save them\n",
    "    total = []\n",
    "    data_split_train = proportions(train_y)\n",
    "    total.append(data_split_train)\n",
    "    data_split_val = proportions(val_y)\n",
    "    total.append(data_split_val)\n",
    "    \n",
    "    with open('10-Fold Validation/Data Splits For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(total,f)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 256,\n",
    "                            stateful = False,\n",
    "                            recurrent_dropout = 0.2,\n",
    "                            activation = 'sigmoid'),\n",
    "                            input_shape = (sample_size,1)))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(15, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy',loss_weights = list(weights), metrics=['accuracy'])\n",
    "    \n",
    "    # Fit to data\n",
    "    \n",
    "    history = model.fit(train_x, train_y, verbose = 1, batch_size = batch_size, validation_data = (val_x, val_y), epochs = 100, shuffle = True)\n",
    "    \n",
    "    # Save history for each fold\n",
    "    with open('10-Fold Validation/History For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(history,f)\n",
    "    \n",
    "    # Save the evaluate values for later\n",
    "    print(\"Evaluation on Training set\")\n",
    "    \n",
    "    train_scores = model.evaluate(train_x, train_y, batch_size = batch_size)\n",
    "    scores[i - 1][0] = train_scores[0]\n",
    "    scores[i - 1][1] = train_scores[1]\n",
    "    #print(train_scores)\n",
    "    \n",
    "    print(\"Evaluation on Validation set\")\n",
    "    \n",
    "    val_scores = model.evaluate(val_x, val_y, batch_size = batch_size)\n",
    "    scores[i - 1][2] = val_scores[0]\n",
    "    scores[i - 1][3] = val_scores[1]\n",
    "    predictions = model.predict(val_x)\n",
    "    # Turn to prediction multilabel outputs\n",
    "    predictions[predictions>=0.5] = 1\n",
    "    predictions[predictions<0.5] = 0\n",
    "    #print(predictions[0])\n",
    "    # Add in hamming loss, jaccard score, recall, precision, f1 score, then save multilabel confusion matrix\n",
    "    scikit_scores = []\n",
    "    scikit_scores.append(hamming_loss(val_y, predictions))\n",
    "    scikit_scores.append(jaccard_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(recall_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(precision_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(f1_score(val_y, predictions, average = None))\n",
    "    confusion = multilabel_confusion_matrix(val_y, predictions)\n",
    "    print(confusion)\n",
    "    #print(val_scores)\n",
    "    #print(scikit_scores)\n",
    "    #print(confusion)\n",
    "    \n",
    "    # Save the scikit parameters for this fold\n",
    "    with open('10-Fold Validation/Scikit Scores For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(scikit_scores,f)\n",
    "    \n",
    "    # Save the confusion matrix for this fold\n",
    "    with open('10-Fold Validation/Confusion Matrix For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(confusion,f)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "np.save('10-Fold Validation/Scores_Weighted.npy', scores, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean = (scores[0][1] + scores[1][1] + scores[2][1]) / 3\n",
    "#print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
