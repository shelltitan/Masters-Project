{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Code Outline -------- #\n",
    "# This code creates the RNN model, trains it using moving\n",
    "# Window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Used to open pickle files and plot figures\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# These all used to create neural network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "# These are used to shuffle the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# These used to open all the files at once into one large list of arrays\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a sequential, bidiimport picklerectional LSTM network\n",
    "\n",
    "# Set number of epochs\n",
    "n_epoch = 30\n",
    "\n",
    "# We use stateful as false as this tells us that\n",
    "# Subsequent batches are not independent\n",
    "stateful = False\n",
    "\n",
    "# -------- Set Tensorflow to run on gpu -------- #\n",
    "# If possible run on GPU as it's quicker however, we couldn't\n",
    "# achieve this with the computers available to us. On a good computer\n",
    "# Via CPU it will take roughly a day to train the RNN\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(device_count={'GPU' : 1, 'CPU': 4})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- RNN Model Creation -------- #\n",
    "# - LSTM Input Format:\n",
    "#   Samples: One window is one sample. A batch is comprised of one or more windows. - 1024 passed in a time\n",
    "#   Time Steps. One time step is one point of observation in the sample.\n",
    "#   Features = 1: As each timestep has just one associated value (Whether there is AF present or not)\n",
    "\n",
    "def createModel(batch_size, time_steps, features):\n",
    "    initializer = \"glorot_uniform\" # xavier normal\n",
    "    model = Sequential();\n",
    "    # output shape = (batch_size, timesteps, units)\n",
    "    # batch size specifies the number of SUBSEQUENT windows passed per training step\n",
    "    model.add(Bidirectional(LSTM(units=200,\n",
    "                                 return_sequences=True, # Many to many layer output\n",
    "                                 stateful=stateful, # Subsequent batches are not independant\n",
    "                                 # Reinitalize subsequent training steps with previous information\n",
    "                                 # Overcomes memory limits in feeding in data\n",
    "                                 recurrent_dropout=0.1,\n",
    "                                 activation='sigmoid',\n",
    "                                 bias_initializer=initializer,\n",
    "                                 kernel_initializer=initializer),\n",
    "                                 merge_mode=\"concat\", batch_input_shape = (batch_size, time_steps, features))) # LSTM Dropout\n",
    "    model.add(GlobalMaxPool1D())#input_shape=(400, 100)))\n",
    "    model.add(Dense(units=50, bias_initializer=initializer, kernel_initializer=initializer, activation=\"relu\", input_shape=(100, 400)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\", bias_initializer=initializer, kernel_initializer=initializer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Training Function -------- #\n",
    "# This function creates the RNN using above function\n",
    "# It then trains the RNN and saves the model and history for analysis\n",
    "\n",
    "# Time steps set to 80 for us as we didn't vary window length\n",
    "def runLSTM(regularised_data, regularised_labels, batch_size_train = 1024, time_steps = 80, features = 1):\n",
    "    # create LSTM\n",
    "    train_model = createModel(batch_size_train, time_steps, features)\n",
    "    # Print out what the models architecture is \n",
    "    print(train_model.summary())\n",
    "    # Use Adam optimizer as per paper\n",
    "    opt = keras.optimizers.Adam(lr=0.001)  #, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    # Binary_crossentropy as per paper - because binary classification\n",
    "    train_model.compile(optimizer=opt, loss = 'binary_crossentropy', metrics=[\"binary_accuracy\", \"mae\"])#metrics = ['mse', 'mae', 'accuracy'])\n",
    "\n",
    "    ## -- Draw Model Structure -- ##\n",
    "#     from keras.utils import plot_model\n",
    "#     plot_model(train_model, to_file='blstm.pdf', show_shapes=True, show_layer_names=False, rankdir='TB')\n",
    "\n",
    "    print(\"Preparing data for window size \" + str(time_steps) +\"\\n-----------------\")\n",
    "    \n",
    "    ## -- Train the LSTM in Batches -- ##\n",
    "    print(\"Initializing training for window size \" + str(time_steps) + \"\\n-----------------\")\n",
    "    \n",
    "    # We split the training data into 15 percent validation, 85% training usage\n",
    "    non_batch_split = 0.15 # 15 percent for validation\n",
    "    val_split = ((len(regularised_data) * non_batch_split) - ((len(regularised_data) * non_batch_split) % batch_size_train)) / len(regularised_data)\n",
    "#     val_split = non_batch_split - ((non_batch_split * len(x) % batch_size_train) / len(x))\n",
    "    history = train_model.fit(regularised_data, regularised_labels, verbose=True, batch_size = batch_size_train, shuffle=True, epochs=n_epoch, validation_split=val_split)# validation_data=(test_x, test_y))\n",
    "    print(\"Finished Training\\n-----------------\")\n",
    "\n",
    "    ## -- Save History for analysis -- ##\n",
    "    with open(\"output/training_history-\" + str(time_steps) + \"-final.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    print(\"Saving model with window size \" + str(time_steps) + \"\\n-----------------\")\n",
    "\n",
    "    ## -- Save the model -- ##\n",
    "    train_model.save('saved_model/final-model.h5')\n",
    "\n",
    "    print(\"Finished training and saving model with window size \" + str(time_steps) + \"\\n-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Loader -------- #\n",
    "# Need to open up all the window files and save them into a list\n",
    "# OS opens all the files in the RR Window Arrays folder\n",
    "# These are then appended into one list\n",
    "# To a numpy array using array()\n",
    "\n",
    "RR_window_arrays = []\n",
    "\n",
    "files = os.listdir('RR Window Arrays/')\n",
    "    \n",
    "for file in files:\n",
    "    path = 'RR Window Arrays/' + file\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    RR_window_arrays.append(data)\n",
    "        \n",
    "RR_window_labels = []\n",
    "    \n",
    "# Now do the same with the labels\n",
    "\n",
    "files = os.listdir('RR Window Labels/')\n",
    "    \n",
    "for file in files:\n",
    "    path = 'RR Window Labels/' + file\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    RR_window_labels.append(data)\n",
    "    \n",
    "# print(RR_window_arrays[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Data Adjustment for RR Window Arrays -------- #\n",
    "\n",
    "# Need input data to be of the form (1024, 80, 1) for the window arrays\n",
    "# Turn these data sets into numpy arrays then reshape\n",
    "\n",
    "# Array (21,) of 21 random arrays with different dimensions (N, 80)\n",
    "RR_window_arrays = np.array([np.array(xi) for xi in RR_window_arrays])\n",
    "# print(RR_window_arrays[20][60007])\n",
    "# print(RR_window_arrays[1].shape[0])\n",
    "number_of_points = 0\n",
    "# Loop over every individual array out of the 21 and give a 3rd dimension\n",
    "for c,i in enumerate(RR_window_arrays):\n",
    "    RR_window_arrays[c] = np.reshape(RR_window_arrays[c], (RR_window_arrays[c].shape[0], 80, 1))\n",
    "    number_of_points += RR_window_arrays[c].shape[0]\n",
    "    \n",
    "# print(number_of_points)\n",
    "window_arrays = np.zeros((number_of_points, 80, 1))\n",
    "# print(len(window_arrays[0]))\n",
    "\n",
    "index = 0\n",
    "# Now vertically stack all the 21 arrays so that we have a big array of form (K, 80,1)\n",
    "for c,i in enumerate(RR_window_arrays):\n",
    "    if (c == 0):\n",
    "        continue\n",
    "    if (c == 1):\n",
    "        window_arrays[0:(len(RR_window_arrays[c - 1]) + len(RR_window_arrays[c]))] = np.vstack((RR_window_arrays[c - 1], RR_window_arrays[c]))\n",
    "        # Save the index for what splice we are on\n",
    "        index = (len(RR_window_arrays[c - 1]) + len(RR_window_arrays[c]))\n",
    "    if ((c != 1) and (c != 0) and (c % 2 == 0) and (c != (len(RR_window_arrays) - 1))):\n",
    "        continue\n",
    "    if ((c != 1) and (c != 0) and (c % 2 != 0)):\n",
    "        window_arrays[index:(index + (len(RR_window_arrays[c - 1]) + len(RR_window_arrays[c])))] = np.vstack((RR_window_arrays[c - 1], RR_window_arrays[c]))\n",
    "        index += (len(RR_window_arrays[c - 1]) + len(RR_window_arrays[c]))\n",
    "    if (c == (len(RR_window_arrays) - 1)):\n",
    "        window_arrays = np.vstack((window_arrays[:index], RR_window_arrays[c]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.36133326]\n",
      "  [-1.82322223]\n",
      "  [ 1.26194768]\n",
      "  [-2.25371106]\n",
      "  [-1.64385189]\n",
      "  [ 1.15432548]\n",
      "  [ 0.29334783]\n",
      "  [ 0.22159969]\n",
      "  [ 0.29334783]\n",
      "  [ 0.14985155]\n",
      "  [ 0.11397748]\n",
      "  [-2.39720733]\n",
      "  [-2.39720733]\n",
      "  [-2.86357023]\n",
      "  [-3.61692567]\n",
      "  [-3.11468871]\n",
      "  [-3.04294057]\n",
      "  [ 0.86733293]\n",
      "  [ 0.47271817]\n",
      "  [-2.46895547]\n",
      "  [-1.71560003]\n",
      "  [ 1.33369582]\n",
      "  [-2.28958513]\n",
      "  [-1.46448155]\n",
      "  [ 1.08257734]\n",
      "  [ 0.54446631]\n",
      "  [ 0.32922189]\n",
      "  [ 0.4368441 ]\n",
      "  [ 0.40097003]\n",
      "  [ 0.22159969]\n",
      "  [-2.50482954]\n",
      "  [-2.28958513]\n",
      "  [-2.86357023]\n",
      "  [-3.61692567]\n",
      "  [-3.32993312]\n",
      "  [-3.50930347]\n",
      "  [-3.54517753]\n",
      "  [-3.43755533]\n",
      "  [-3.79629602]\n",
      "  [-1.35685934]\n",
      "  [-2.82769616]\n",
      "  [-4.33440705]\n",
      "  [-3.15056278]\n",
      "  [-3.36580719]\n",
      "  [-4.01154043]\n",
      "  [-4.1550367 ]\n",
      "  [-2.8994443 ]\n",
      "  [-2.21783699]\n",
      "  [-3.65279974]\n",
      "  [-2.25371106]\n",
      "  [ 1.5848143 ]\n",
      "  [ 0.61621444]\n",
      "  [ 0.36509596]\n",
      "  [-2.57657768]\n",
      "  [ 3.37851774]\n",
      "  [-2.3254592 ]\n",
      "  [ 3.01977705]\n",
      "  [-2.39720733]\n",
      "  [-2.25371106]\n",
      "  [ 1.11845141]\n",
      "  [-2.39720733]\n",
      "  [-0.53175576]\n",
      "  [ 1.11845141]\n",
      "  [-2.3254592 ]\n",
      "  [-1.71560003]\n",
      "  [ 1.0108292 ]\n",
      "  [ 0.61621444]\n",
      "  [ 0.29334783]\n",
      "  [ 0.18572562]\n",
      "  [ 0.22159969]\n",
      "  [ 0.14985155]\n",
      "  [ 0.22159969]\n",
      "  [ 0.14985155]\n",
      "  [ 0.04222934]\n",
      "  [ 0.07810341]\n",
      "  [ 0.14985155]\n",
      "  [ 0.11397748]\n",
      "  [ 0.14985155]\n",
      "  [ 0.04222934]\n",
      "  [ 0.07810341]]\n",
      "\n",
      " [[-1.82322223]\n",
      "  [ 1.26194768]\n",
      "  [-2.25371106]\n",
      "  [-1.64385189]\n",
      "  [ 1.15432548]\n",
      "  [ 0.29334783]\n",
      "  [ 0.22159969]\n",
      "  [ 0.29334783]\n",
      "  [ 0.14985155]\n",
      "  [ 0.11397748]\n",
      "  [-2.39720733]\n",
      "  [-2.39720733]\n",
      "  [-2.86357023]\n",
      "  [-3.61692567]\n",
      "  [-3.11468871]\n",
      "  [-3.04294057]\n",
      "  [ 0.86733293]\n",
      "  [ 0.47271817]\n",
      "  [-2.46895547]\n",
      "  [-1.71560003]\n",
      "  [ 1.33369582]\n",
      "  [-2.28958513]\n",
      "  [-1.46448155]\n",
      "  [ 1.08257734]\n",
      "  [ 0.54446631]\n",
      "  [ 0.32922189]\n",
      "  [ 0.4368441 ]\n",
      "  [ 0.40097003]\n",
      "  [ 0.22159969]\n",
      "  [-2.50482954]\n",
      "  [-2.28958513]\n",
      "  [-2.86357023]\n",
      "  [-3.61692567]\n",
      "  [-3.32993312]\n",
      "  [-3.50930347]\n",
      "  [-3.54517753]\n",
      "  [-3.43755533]\n",
      "  [-3.79629602]\n",
      "  [-1.35685934]\n",
      "  [-2.82769616]\n",
      "  [-4.33440705]\n",
      "  [-3.15056278]\n",
      "  [-3.36580719]\n",
      "  [-4.01154043]\n",
      "  [-4.1550367 ]\n",
      "  [-2.8994443 ]\n",
      "  [-2.21783699]\n",
      "  [-3.65279974]\n",
      "  [-2.25371106]\n",
      "  [ 1.5848143 ]\n",
      "  [ 0.61621444]\n",
      "  [ 0.36509596]\n",
      "  [-2.57657768]\n",
      "  [ 3.37851774]\n",
      "  [-2.3254592 ]\n",
      "  [ 3.01977705]\n",
      "  [-2.39720733]\n",
      "  [-2.25371106]\n",
      "  [ 1.11845141]\n",
      "  [-2.39720733]\n",
      "  [-0.53175576]\n",
      "  [ 1.11845141]\n",
      "  [-2.3254592 ]\n",
      "  [-1.71560003]\n",
      "  [ 1.0108292 ]\n",
      "  [ 0.61621444]\n",
      "  [ 0.29334783]\n",
      "  [ 0.18572562]\n",
      "  [ 0.22159969]\n",
      "  [ 0.14985155]\n",
      "  [ 0.22159969]\n",
      "  [ 0.14985155]\n",
      "  [ 0.04222934]\n",
      "  [ 0.07810341]\n",
      "  [ 0.14985155]\n",
      "  [ 0.11397748]\n",
      "  [ 0.14985155]\n",
      "  [ 0.04222934]\n",
      "  [ 0.07810341]\n",
      "  [ 0.18572562]]]\n",
      "[0 0]\n",
      "(1032545, 80, 1)\n",
      "(1032545,)\n",
      "(774408, 80, 1)\n",
      "(774408,)\n",
      "(258137, 80, 1)\n",
      "(258137,)\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (1024, 80, 400)           323200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (1024, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1024, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (1024, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1024, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 343,301\n",
      "Trainable params: 343,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Preparing data for window size 80\n",
      "-----------------\n",
      "Initializing training for window size 80\n",
      "-----------------\n",
      "Train on 658432 samples, validate on 115712 samples\n",
      "Epoch 1/30\n",
      "658432/658432 [==============================] - 1832s 3ms/step - loss: 0.2930 - binary_accuracy: 0.8710 - mean_absolute_error: 0.2026 - val_loss: 0.1107 - val_binary_accuracy: 0.9621 - val_mean_absolute_error: 0.0654\n",
      "Epoch 2/30\n",
      "658432/658432 [==============================] - 1884s 3ms/step - loss: 0.0972 - binary_accuracy: 0.9649 - mean_absolute_error: 0.0585 - val_loss: 0.0723 - val_binary_accuracy: 0.9747 - val_mean_absolute_error: 0.0436\n",
      "Epoch 3/30\n",
      "658432/658432 [==============================] - 1887s 3ms/step - loss: 0.0702 - binary_accuracy: 0.9753 - mean_absolute_error: 0.0411 - val_loss: 0.0533 - val_binary_accuracy: 0.9814 - val_mean_absolute_error: 0.0290\n",
      "Epoch 4/30\n",
      "658432/658432 [==============================] - 1830s 3ms/step - loss: 0.0559 - binary_accuracy: 0.9809 - mean_absolute_error: 0.0321 - val_loss: 0.0486 - val_binary_accuracy: 0.9833 - val_mean_absolute_error: 0.0266\n",
      "Epoch 5/30\n",
      "658432/658432 [==============================] - 1840s 3ms/step - loss: 0.0465 - binary_accuracy: 0.9844 - mean_absolute_error: 0.0265 - val_loss: 0.0375 - val_binary_accuracy: 0.9874 - val_mean_absolute_error: 0.0202\n",
      "Epoch 6/30\n",
      "658432/658432 [==============================] - 1847s 3ms/step - loss: 0.0394 - binary_accuracy: 0.9869 - mean_absolute_error: 0.0223 - val_loss: 0.0316 - val_binary_accuracy: 0.9895 - val_mean_absolute_error: 0.0164\n",
      "Epoch 7/30\n",
      "658432/658432 [==============================] - 1900s 3ms/step - loss: 0.0347 - binary_accuracy: 0.9885 - mean_absolute_error: 0.0196 - val_loss: 0.0271 - val_binary_accuracy: 0.9914 - val_mean_absolute_error: 0.0151\n",
      "Epoch 8/30\n",
      "658432/658432 [==============================] - 2007s 3ms/step - loss: 0.0303 - binary_accuracy: 0.9900 - mean_absolute_error: 0.0170 - val_loss: 0.0236 - val_binary_accuracy: 0.9923 - val_mean_absolute_error: 0.0133\n",
      "Epoch 9/30\n",
      "658432/658432 [==============================] - 2040s 3ms/step - loss: 0.0262 - binary_accuracy: 0.9914 - mean_absolute_error: 0.0147 - val_loss: 0.0201 - val_binary_accuracy: 0.9931 - val_mean_absolute_error: 0.0100\n",
      "Epoch 10/30\n",
      "147456/658432 [=====>........................] - ETA: 25:18 - loss: 0.0247 - binary_accuracy: 0.9920 - mean_absolute_error: 0.0137"
     ]
    }
   ],
   "source": [
    "# -------- Data Adjustment For Label Array -------- #\n",
    "# We need the labels to be one big list of shape (k,)\n",
    "# Therefore, re-shape the original label array first\n",
    "\n",
    "np.reshape(labels, (len(labels), 1))\n",
    "window_labels = np.full((number_of_points,), 7)\n",
    "# print(labels[0].shape)\n",
    "\n",
    "# Now horizontally stack all the 21 arrays so that we get the form (k,)\n",
    "for c,i in enumerate(labels):\n",
    "    if (c == 0):\n",
    "        continue\n",
    "    if (c == 1):\n",
    "        window_labels[0:(len(labels[c - 1]) + len(labels[c]))] = np.hstack((labels[c - 1], labels[c]))\n",
    "        # Save the index for what splice we are on\n",
    "        index = (len(labels[c - 1]) + len(labels[c]))\n",
    "    if ((c != 1) and (c != 0) and (c % 2 == 0) and (c != (len(labels) - 1))):\n",
    "        continue\n",
    "    if ((c != 1) and (c != 0) and (c % 2 != 0)):\n",
    "        window_labels[index:(index + (len(labels[c - 1]) + len(labels[c])))] = np.hstack((labels[c - 1], labels[c]))\n",
    "        index += (len(labels[c - 1]) + len(labels[c]))\n",
    "    if (c == (len(labels) - 1)):\n",
    "        window_labels = np.hstack((window_labels[:index], labels[c]))\n",
    "\n",
    "# Now that we have the data, we want to shuffle the data set\n",
    "# Then split into training and testing\n",
    "\n",
    "print(window_arrays[:2])\n",
    "print(window_labels[:2])\n",
    "print(window_arrays.shape)\n",
    "print(window_labels.shape)\n",
    "\n",
    "# Scipy function that shuffles the data as well as separating into test and training splits\n",
    "x_train, x_test, y_train, y_test = train_test_split(window_arrays, window_labels, test_size=0.25, shuffle = True)\n",
    "    \n",
    "# Check these are shuffled correctly and still right dimension\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Now we need to split the training set into batches so we need to truncate the data\n",
    "# To make sure it is dimensionally possible\n",
    "number_batches_possible = y_train.shape[0] / 1024\n",
    "# print(number_batches_possible)\n",
    "\n",
    "number_batches_over = number_batches_possible - round(number_batches_possible)\n",
    "# print(number_batches_over)\n",
    "\n",
    "amount_of_windows = (1024 * round(number_batches_possible))\n",
    "# print(amount_of_windows)\n",
    "\n",
    "# Now how ever much 'excess' train data we don't need, we can switch this into the test set\n",
    "\n",
    "final_x_train = x_train[:amount_of_windows]\n",
    "final_y_train = y_train[:amount_of_windows]\n",
    "\n",
    "final__x_test = np.concatenate((x_test, x_train[amount_of_windows:]))\n",
    "final__y_test = np.concatenate((y_test, y_train[amount_of_windows:]))\n",
    "\n",
    "# Now we have test and train data we can input into the model\n",
    "\n",
    "runLSTM(final_x_train, final_y_train, batch_size_train=1024, time_steps=80, features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Saver -------- #\n",
    "# We export the test set and training set values so that we can calculate\n",
    "# Specificities and sensitivities of the model\n",
    "\n",
    "# filename = 'Window_80_Test_Set_y'\n",
    "# This line uses pickle to save the window arrays as a .pkl file\n",
    "\n",
    "# with open('{}.pkl'.format(filename), 'wb') as f:\n",
    "#      pickle.dump(final__y_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
